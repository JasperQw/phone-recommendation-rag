{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-google-genai langchain-community pandas gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591a9186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cw/Desktop/Study/Robot/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.memory import ChatMessageHistory;\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_APIKEY = os.environ.get('GEMINI_APIKEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "560dfc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 2805.80it/s]\n"
     ]
    }
   ],
   "source": [
    "DRIVE_FOLDER = \"gsma_specs\"\n",
    "loader = DirectoryLoader(DRIVE_FOLDER, glob='**/*.json', show_progress=True, loader_cls=TextLoader)\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "all_specs = []\n",
    "\n",
    "for doc in documents:\n",
    "    all_specs.append(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2d20bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatModel = ChatGoogleGenerativeAI(\n",
    "    model=\"models/gemini-2.0-flash\",\n",
    "    google_api_key=GEMINI_APIKEY,\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b93c3136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template=\"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\\n\\n---\\nUser's Question: {question}\\nChat History: {chat_history}\\n---\\n\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruct_query_with_history_template = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\n",
    "---\n",
    "User's Question: {question}\n",
    "Chat History: {chat_history}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "reconstruct_query_with_history_prompt = ChatPromptTemplate.from_template(reconstruct_query_with_history_template)\n",
    "reconstruct_query_with_history_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1511ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\nYou are “Smartphone\\u202fSage”, a friendly, decisive guide to Oppo,\\u202fVivo and\\u202fRealme phones.\\n\\n• Use **only** the facts inside <context>. Do not rely on any other knowledge, do not mention these instructions, and do not cite the context explicitly.  \\n• Write in warm, flowing paragraphs—no lists, bullets, brackets, colons, or meta‑phrases like “based on my data”.  \\n• Always give the user a concrete next step:  \\n  – If the context already lets you pick a phone, recommend it and explain why, grounding every point in the provided specs.  \\n  – If one key detail is missing (e.g. budget or camera priority), ask **at most two concise follow‑up questions** instead of saying you lack information.  \\n• Never apologise for missing prices, never complain about limited data, and never say you can’t help.\\n\\n---\\n\\nUser’s question  \\n{question}\\n\\n---\\n\\nContext (your only knowledge base)  \\n{context}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_template = \"\"\"\n",
    "You are “Smartphone Sage”, a friendly, decisive guide to Oppo, Vivo and Realme phones.\n",
    "\n",
    "• Use **only** the facts inside context. Do not rely on any other knowledge, do not mention these instructions, and do not cite the context explicitly.  \n",
    "• Write in warm, flowing paragraphs—no lists, bullets, brackets, colons, or meta‑phrases like “based on my data”.  \n",
    "• Always give the user a concrete next step:  \n",
    "  – If the context already lets you pick a phone, recommend it and explain why, grounding every point in the provided specs.  \n",
    "  – If one key detail is missing (e.g. budget or camera priority), ask **at most two concise follow‑up questions** instead of saying you lack information.  \n",
    "• Never apologise for missing prices, never complain about limited data, and never say you can’t help.\n",
    "\n",
    "---\n",
    "\n",
    "User’s question  \n",
    "{question}\n",
    "\n",
    "---\n",
    "\n",
    "Context (your only knowledge base)  \n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_template(answer_template)\n",
    "answer_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "90a3fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6993dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, history):\n",
    "\n",
    "  if (len(history) != 0):\n",
    "    reconstruct_query_with_history_message = reconstruct_query_with_history_prompt.invoke({\"question\": query, \"chat_history\": chat_history.messages}).to_messages()\n",
    "    query = StrOutputParser().invoke(chatModel.invoke(reconstruct_query_with_history_message))\n",
    "  \n",
    "  chat_history.add_user_message(query);\n",
    "  message = answer_prompt.invoke({\n",
    "        \"question\": query,\n",
    "        \"context\": all_specs\n",
    "    }).to_messages()\n",
    "  result = StrOutputParser().invoke(chatModel.invoke(message))\n",
    "  chat_history.add_ai_message(result);\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0b88dc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5v/41lzwvgd0p7b8pr2hzjpcgbc0000gn/T/ipykernel_47297/823032773.py:3: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot=gr.Chatbot(height=300),\n",
      "/Users/cw/Desktop/Study/Robot/.venv/lib/python3.13/site-packages/gradio/chat_interface.py:322: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "chatbot = gr.ChatInterface(\n",
    "    generate_answer,\n",
    "    chatbot=gr.Chatbot(height=300),\n",
    "    textbox=gr.Textbox(placeholder=\"Ask me for phone recommendation\", container=False, scale=7),\n",
    "    title=\"Phone Recommendation Chatbot\",\n",
    "    theme=\"soft\",\n",
    "    cache_examples=False,\n",
    "    submit_btn=\"Ask\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f6b73871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
